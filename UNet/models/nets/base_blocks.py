import torch
import torch.nn as nn
from typing import Optional, Sequence, Tuple, Union

from monai.networks.blocks.dynunet_block import UnetBasicBlock, UnetResBlock
# from monai.networks.layers.factories import Act, Norm
from monai.networks.blocks.dynunet_block import get_conv_layer

from UNet.models.nets.utils import get_multi_conv_layer
from UNet.models.nets.blocks_MDUNet import conv_3_1, DualDilatedBlock


class MultiUnetBasicBlock(nn.Module):

    def __init__(
            self,
            in_channels: int,
            out_channels: int,
            multiS_conv=True,
            act='relu',
    ):
        super().__init__()

        self.cov_ = conv_3_1(ch_in=in_channels, ch_out=out_channels, act=act) if multiS_conv else DualDilatedBlock(
            ch_in=in_channels, ch_out=out_channels, act=act)

    def forward(self, inp):
        out = self.cov_(inp)
        return out


class MultiUnetResBlock(UnetResBlock):
    def __init__(
            self,
            spatial_dims: int,
            in_channels: int,
            out_channels: int,
            kernel_size: Union[Sequence[int], int],
            stride: Union[Sequence[int], int],
            norm_name: Union[Tuple, str],
            act_name: Union[Tuple, str] = ("leakyrelu", {"inplace": True, "negative_slope": 0.01}),
            dropout: Optional[Union[Tuple, str, float]] = None,
    ):
        super().__init__(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            norm_name=norm_name,
        )

        self.conv1 = get_multi_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=stride,
            dropout=dropout,
            act=act_name,
            norm=norm_name,
            conv_only=False,
        )
        self.conv2 = get_multi_conv_layer(
            spatial_dims,
            out_channels,
            out_channels,
            kernel_size=kernel_size,
            stride=1,
            dropout=dropout,
            act=None,
            norm=None,
            conv_only=False,
        )

    def forward(self, inp):
        residual = inp
        out = self.conv1(inp)
        out = self.norm1(out)
        out = self.lrelu(out)
        out = self.conv2(out)
        out = self.norm2(out)
        if hasattr(self, "conv3"):
            residual = self.conv3(residual)
        if hasattr(self, "norm3"):
            residual = self.norm3(residual)
        out += residual
        out = self.lrelu(out)
        return out


class ModifiedUnetrUpBlock(nn.Module):
    """
    An upsampling module that can be used for UNETR: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(
            self,
            spatial_dims: int,
            in_channels: int,
            out_channels: int,
            upsample_kernel_size: Union[Sequence[int], int],
            act='relu',
            norm='instance',
            multiS_conv=True
    ) -> None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
        """

        super().__init__()
        upsample_stride = upsample_kernel_size
        self.act = act
        self.transp_conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
            norm=norm
        )

        self.conv_block = MultiUnetBasicBlock(  # type: ignore
            out_channels + out_channels,
            out_channels,
            act=act,
            multiS_conv=multiS_conv,
        )

    def forward(self, inp, skip):
        # number of channels for skip should equals to out_channels
        out = self.transp_conv(inp)
        out = torch.cat((out, skip), dim=1)
        out = self.conv_block(out)
        return out


class ModifiedUnetOutBlock(nn.Module):

    def __init__(
            self, spatial_dims: int, in_channels: int, out_channels: int,
            dropout: Optional[Union[Tuple, str, float]] = None
    ):
        super().__init__()
        self.conv = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=1,
            stride=1,
            dropout=dropout,
            bias=True,
            act=None,
            norm=None,
            conv_only=False,
        )

    def forward(self, inp):
        return self.conv(inp)


class ModifiedUnetrPrUpBlock(nn.Module):
    """
    A projection upsampling module that can be used for UNETR: "Hatamizadeh et al.,
    UNETR: Transformers for 3D Medical Image Segmentation <https://arxiv.org/abs/2103.10504>"
    """

    def __init__(
            self,
            spatial_dims: int,
            in_channels: int,
            out_channels: int,
            num_layer: int,
            kernel_size: Union[Sequence[int], int],
            stride: Union[Sequence[int], int],
            upsample_kernel_size: Union[Sequence[int], int],
            norm_name: Union[Tuple, str],
            conv_block: bool = False,
            res_block: bool = False,
    ) -> None:
        """
        Args:
            spatial_dims: number of spatial dimensions.
            in_channels: number of input channels.
            out_channels: number of output channels.
            num_layer: number of upsampling blocks.
            kernel_size: convolution kernel size.
            stride: convolution stride.
            upsample_kernel_size: convolution kernel size for transposed convolution layers.
            norm_name: feature normalization type and arguments.
            conv_block: bool argument to determine if convolutional block is used.
            res_block: bool argument to determine if residual block is used.
        """

        super().__init__()

        upsample_stride = upsample_kernel_size
        self.transp_conv_init = get_conv_layer(
            spatial_dims,
            in_channels,
            out_channels,
            kernel_size=upsample_kernel_size,
            stride=upsample_stride,
            conv_only=True,
            is_transposed=True,
        )
        if conv_block:
            if res_block:
                self.blocks = nn.ModuleList(
                    [
                        nn.Sequential(
                            get_conv_layer(
                                spatial_dims,
                                out_channels,
                                out_channels,
                                kernel_size=upsample_kernel_size,
                                stride=upsample_stride,
                                conv_only=True,
                                is_transposed=True,
                            ),
                            UnetResBlock(
                                spatial_dims=spatial_dims,
                                in_channels=out_channels,
                                out_channels=out_channels,
                                kernel_size=kernel_size,
                                stride=stride,
                                norm_name=norm_name,
                            ),
                        )
                        for _ in range(num_layer)
                    ]
                )
            else:
                self.blocks = nn.ModuleList(
                    [
                        nn.Sequential(
                            get_conv_layer(
                                spatial_dims,
                                out_channels,
                                out_channels,
                                kernel_size=upsample_kernel_size,
                                stride=upsample_stride,
                                conv_only=True,
                                is_transposed=True,
                            ),
                            UnetBasicBlock(
                                spatial_dims=spatial_dims,
                                in_channels=out_channels,
                                out_channels=out_channels,
                                kernel_size=kernel_size,
                                stride=stride,
                                norm_name=norm_name,
                            ),
                        )
                        for _ in range(num_layer)
                    ]
                )
        else:
            self.blocks = nn.ModuleList(
                [
                    get_conv_layer(
                        spatial_dims,
                        out_channels,
                        out_channels,
                        kernel_size=upsample_kernel_size,
                        stride=upsample_stride,
                        conv_only=True,
                        is_transposed=True,
                    )
                    for _ in range(num_layer)
                ]
            )

    def forward(self, x):
        x = self.transp_conv_init(x)
        return x


def test():
    feature_size: int = 16
    hidden_size: int = 768
    norm_name: Union[Tuple, str] = "instance"
    res_block: bool = True
    spatial_dims: int = 3
    out_channels = 3
    # model = ModifiedUnetrUpBlock(
    #     spatial_dims=spatial_dims,
    #     in_channels=2,
    #     out_channels=3,
    #     kernel_size=3,
    #     upsample_kernel_size=2,
    #     norm_name=norm_name,
    #     res_block=res_block, )
    model = ModifiedUnetrPrUpBlock(
        spatial_dims=spatial_dims,
        in_channels=hidden_size,
        out_channels=feature_size * 2,
        num_layer=2,
        kernel_size=3,
        stride=1,
        upsample_kernel_size=2,
        norm_name=norm_name,
        conv_block=True,
        res_block=True,
    )
    vol1 = torch.randn((1, 768, 64, 64, 64))
    vol2 = torch.randn((1, 3, 128, 128, 128))
    # out.shape : (1, 3, 128, 128, 128)

    # pred = model(vol1, vol2)
    pred = model(vol1)
    print(pred.shape)


if __name__ == '__main__':
    test()
